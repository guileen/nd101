{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络入门"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "算法描述\n",
    "\n",
    "* 训练每一条数据\n",
    "    * 对每一层,直到最后一层\n",
    "        * 计算 $ h = \\sum_i w_i x_i $\n",
    "        * 计算输出 $ \\hat y = f(h) $\n",
    "    * 计算 Error term, $\\delta = ( y - \\hat y)f'(h) $\n",
    "    * $ \\delta = (y - \\hat y)f'(h) $\n",
    "    * 从后向前，对每一层\n",
    "        * $ \\delta_j^h = \\sum{W_{jk}\\delta_k^o f'(h_j)} $\n",
    "        * $ \\Delta w_{ij} += \\delta_j^h x_i $\n",
    "\n",
    "* 更新 $ w = w + \\eta \\Delta w / m $。 $\\eta$ 是学习率 $m$ 是数据条数。对权重步长做了平均化，防止数据起伏\n",
    "* 重复 $e$ 代 (epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/07e338ce-41fa-4b2a-b1b9-5997261c3f58)\n",
    "\n",
    "这些独立的节点被称作感知器 或者神经元。它们是构成神经网络的基本单元。\n",
    "\n",
    "Weights（权值）\n",
    "\n",
    "当输入给到节点，激活函数可以决定节点的输出。因为它决定了实际输出，我们也把层的输出，称作“激活”。\n",
    "\n",
    "是单位阶跃函数（Heaviside step function）。如果线性组合小于0，函数返回0，如果线性组合等于或者大于0，函数返回1\n",
    "\n",
    "偏置 bias \n",
    "\n",
    "## 逻辑操作激活函数\n",
    "\n",
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/0aa1e0d3-8440-41b7-b327-925472eaf72e)\n",
    "\n",
    "通过调整w,b 可以使用 w*X + b > 0 实现 AND OR NOT 操作\n",
    "* AND w1 = 1, w2 = 1, b = -2\n",
    "* OR  w1 = 1, w2 = 1, b = -2.1\n",
    "* NOT  w = -1，bias = 0.9\n",
    "\n",
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/4fb8d10d-5f1b-4557-85ad-6421d5eafafe)\n",
    "* XOR 用上图构建 A NOT; B AND; C OR\n",
    "\n",
    "## 其他激活函数\n",
    "\n",
    "其它常见激活函数还有对数几率（又称作 sigmoid），tanh 和 softmax。这节课中我们主要使用 sigmoid 函数:\n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1+e^{-x}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 最简单的神经网络\n",
    "\n",
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/e429472f-a8bf-411a-87e5-6abf1223a725)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43290709503454572"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "inputs = np.array([0.7, -0.3])\n",
    "weights = np.array([0.1, 0.8])\n",
    "bias = -0.1\n",
    "\n",
    "output = sigmoid(np.dot(inputs, weights) + bias)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度下降法\n",
    "\n",
    "[梯度下降法的优化](http://sebastianruder.com/optimizing-gradient-descent/index.html#momentum)\n",
    "\n",
    "E 错误，使E最小化，m为样本数，$\\hat y$ 为预测结果\n",
    "\n",
    "$$\\mathbf{ E = \\frac{1}{2m}\\sum_\\mu(y^\\mu-\\hat y^\\mu)^2 }$$\n",
    "\n",
    "\n",
    "基本思想：\n",
    "\n",
    "$$ W_i = W_i + \\Delta W_i $$\n",
    "$$ \\Delta W_i = - \\eta \\frac{\\partial E}{\\partial W_i} $$\n",
    "\n",
    "得出(过程详见下文):\n",
    "$$ \\Delta w_i = \\eta (y - \\hat y)f'(h)x_i $$\n",
    "\n",
    "\n",
    "### 定义“误差项”（ERROR TERM）$\\delta$\n",
    "\n",
    "\\begin{align}\n",
    "& \\LARGE{\\delta = (y - \\hat y)f'(h) ＝ (y - \\hat y)f'(\\sum_i w_i x_i)} \\\\\n",
    "& \\LARGE{w_i = w_i + \\eta \\delta x_i}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多条数据时\n",
    "\n",
    "* 训练每一条数据\n",
    "    * 计算输出 $ \\hat y = f(\\sum_i w_i x_i) $\n",
    "    * 计算 Error term, $\\delta = ( y - \\hat y)f'(\\sum_i w_i x_i) $\n",
    "    * 计算 $\\Delta w_i = \\Delta w_i + \\delta x_i $\n",
    "\n",
    "* 更新 $ w_i = w_i + \\eta \\Delta w_i / m $。 $\\eta$ 是学习率 $m$ 是数据条数。对权重步长做了平均化，防止数据起伏\n",
    "* 重复 $e$ 代 (epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 推导过程：\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial W_i} & = \\frac{\\partial}{\\partial w_i} \\frac{1}{2} (y-\\hat y)^2 \\\\\n",
    "& = (y - \\hat y)\\frac{\\partial}{\\partial w_i}(y - \\hat y) \\\\\n",
    "& = (y - \\hat y)\\frac{\\partial(y - \\hat y)}{\\partial \\hat y}\\frac{\\partial \\hat y}{\\partial w_i} \\\\\n",
    "& = -(y-\\hat y)\\frac{\\partial \\hat y}{\\partial w_i}\n",
    "\\end{align}\n",
    "\n",
    "已知 $ \\hat y = f(h)$ 其中 $f$ 是激活函数，$h$ 是线性组合函数。$h= \\sum_{i=0}w_i x_i $\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial w_i} & = - (y-\\hat y)\\frac{\\partial \\hat y}{\\partial w_i} \\\\\n",
    "& = - (y - \\hat y)\\frac{\\partial \\hat y}{\\partial h} \\frac{\\partial h}{\\partial w_i} \\\\\n",
    "& = - (y - \\hat y)f'(h)\\frac{\\partial}{\\partial w_i}\\sum w_i x_i\n",
    "\\end{align}\n",
    "\n",
    "而：\n",
    "\\begin{align}\n",
    "& \\frac{\\partial}{\\partial w_i} \\sum_i w_i x_i \\\\\n",
    "= & \\frac{\\partial}{\\partial w_1}[w_1 x_1 + w_2 x_2 + ... + w_n x_n] \\\\\n",
    "= & x_1 + 0 + 0 ...\n",
    "\\end{align}\n",
    "\n",
    "得出：\n",
    "$$ \\frac{\\partial}{\\partial w_i}\\sum_i w_i x_i = x_i $$\n",
    "\n",
    "所以：\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial w_i} & = -(y-\\hat y)f'(h)x_i \\\\\n",
    "\\Delta w_i & = \\eta(y - \\hat y)f'(h)x_i\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sigmoid 函数的导数\n",
    "\n",
    "$$ \\sigma'(x) = \\sigma(x)\\cdot(1-\\sigma(x)) $$\n",
    "\n",
    "推导过程：\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma'(x) & = \\frac{d}{dx}\\frac{1}{(1+e^{-x})} \\\\\n",
    "& = \\frac{d}{dx}(1+e^{-x})^{-1} \\\\\n",
    "& = -(1+e^{-x})^{-2} (-e^{-x}) \\\\\n",
    "& = \\frac{e^{-x}}{1+e^{-x}}^2 \\\\\n",
    "& = \\frac{1}{1+e^{-x}}\\cdot\\frac{e^{-x}}{1+e^{-x}} \\\\\n",
    "& = \\frac{1}{1+e^{-x}}\\cdot\\frac{(1+e^{-x})-1}{1+e^{-x}} \\\\\n",
    "& = \\frac{1}{1+e^{-x}}\\cdot(1-\\frac{1}{1+e^{-x}}) \\\\\n",
    "& = \\sigma(x)\\cdot(1-\\sigma(x))\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## y-hat = sigmoid(w.x) 梯度下降计算过程\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Derivative of the sigmoid function\n",
    "# 激活函数的导数\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "x = np.array([0.1, 0.3])\n",
    "y = 0.2\n",
    "weights = np.array([-0.8, 0.5])\n",
    "learnrate = 0.5\n",
    "\n",
    "# the linear combination performed (h in f(h) and f'(h))\n",
    "h = np.dot(x, weights)\n",
    "\n",
    "# y-hat\n",
    "nn_output = sigmoid(h)\n",
    "\n",
    "# output error (y - y-hat)\n",
    "error = y - nn_output\n",
    "\n",
    "# output gradient (f'(h))\n",
    "output_grad = sigmoid_prime(h)\n",
    "\n",
    "# error term (lowercase delta)\n",
    "error_term = error * output_grad\n",
    "\n",
    "# Gradient descent step \n",
    "del_w = learnrate * error_term * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 实现梯度下降\n",
    "\n",
    "我们拿一个研究生学院录取数据来用梯度下降训练一个网络。数据可以在[这里](http://www.ats.ucla.edu/stat/data/binary.csv)找到。数据有三个输入特征，*GRE分数，GPA，和本科院校排名 rank（从1到4）*。数字1代表最好，数字4代表最差。我们的目标是基于这些特征来预测一个学生能否被研究生院录取。\n",
    "\n",
    "### 数据清理\n",
    "\n",
    "#### Dummy Variable\n",
    "\n",
    "rank 是类别特征，数字并不包含任何相对的值。使用 dummy variables 编码，变成4列，对应列为1，其他列为0.\n",
    "\n",
    "#### 归一化 \n",
    "\n",
    "GRE, GPA 这些数据过大，会导致训练速度下降，因为sigmoid函数的导数，在两侧梯度较低，使步长接近于0。过大或过小的数据都容易使$h$落入两侧。因此需要对数据进行归一化，此外，归一化也会使学习率的选择更加统一，基本上选择 0.001 到 0.1 之间的学习率。\n",
    "\n",
    "标准差公式 $ \\sigma = \\sqrt{\\frac{1}{N}\\sum_i(x_i - \\mu)^2} $ 其中平均值 $\\mu = \\frac{\\sum_i(x_i)}{N} $\n",
    "\n",
    "对于有概率事件，标准差公式 $ \\sigma = \\sqrt{\\sum_ip_i(x_i - \\mu)^2} $ 其中平均值 $\\mu = \\sum_ip_ix_i $\n",
    "\n",
    "\n",
    "\n",
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/0c580bc2-b0a9-4952-bfd0-f2ce6093efe8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 初始化\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# 固定随机种子，便于调试\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 数据清理代码\n",
    "\n",
    "admissions = pd.read_csv('nn_data_1.csv')\n",
    "\n",
    "# Make dummy variables for rank\n",
    "data = pd.concat([admissions, pd.get_dummies(admissions['rank'], prefix='rank')], axis=1)\n",
    "data = data.drop('rank', axis=1)\n",
    "\n",
    "# Standarize features\n",
    "for field in ['gre', 'gpa']:\n",
    "    mean, std = data[field].mean(), data[field].std()\n",
    "    data.loc[:,field] = (data[field]-mean)/std\n",
    "    \n",
    "    \n",
    "# Split off random 10% of the data for testing\n",
    "sample = np.random.choice(data.index, size=int(len(data)*0.9), replace=False)\n",
    "data, test_data = data.ix[sample], data.drop(sample)\n",
    "\n",
    "# Split into features and targets\n",
    "features, targets = data.drop('admit', axis=1), data['admit']\n",
    "features_test, targets_test = test_data.drop('admit', axis=1), test_data['admit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train loss: ', 0.24642503863155604)\n",
      "('Train loss: ', 0.19713264867549785)\n",
      "('Train loss: ', 0.19697021249785832)\n",
      "('Train loss: ', 0.1969622865055684)\n",
      "('Train loss: ', 0.1969617694823392)\n",
      "('Train loss: ', 0.19696173349861354)\n",
      "('Train loss: ', 0.1969617309521564)\n",
      "('Train loss: ', 0.1969617307711557)\n",
      "('Train loss: ', 0.19696173075827528)\n",
      "('Train loss: ', 0.1969617307573584)\n",
      "Prediction accuracy: 0.725\n"
     ]
    }
   ],
   "source": [
    "# 梯度下降代码\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "\n",
    "# Initialize weights\n",
    "weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "\n",
    "# Neural Network hyperparameters\n",
    "epochs = 1000\n",
    "learnrate = 0.01\n",
    "\n",
    "for e in range(epochs):\n",
    "    del_w = np.zeros(weights.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        # Loop through all records, x is the input, y is the target\n",
    "\n",
    "        # Note: We haven't included the h variable from the previous\n",
    "        #       lesson. You can add it if you want, or you can calculate\n",
    "        #       the h together with the output\n",
    "\n",
    "        # TODO: Calculate the output\n",
    "        output = np.dot(x, weights)\n",
    "\n",
    "        # TODO: Calculate the error\n",
    "        error = y - sigmoid(output)\n",
    "\n",
    "        # TODO: Calculate the error term\n",
    "        error_term = error * sigmoid_prime(output)\n",
    "\n",
    "        # TODO: Calculate the change in weights for this sample\n",
    "        #       and add it to the total weight change\n",
    "        del_w += error_term * x\n",
    "\n",
    "    # TODO: Update weights using the learning rate and the average change in weights\n",
    "    weights += learnrate * del_w\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        out = sigmoid(np.dot(features, weights))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "tes_out = sigmoid(np.dot(features_test, weights))\n",
    "predictions = tes_out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 多层网络\n",
    "\n",
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/6f15956b-0cf5-4c07-a7b0-db1c8db26653)\n",
    "\n",
    "$$ h_j = \\sum\\limits_iw_{ij}x_i $$\n",
    "\n",
    "$$\n",
    " h_j = \n",
    " \\begin{bmatrix}\n",
    "  w_{11} & w_{12} & w_{13} \\\\\n",
    "  w_{21} & w_{22} & w_{23}\n",
    " \\end{bmatrix}\n",
    " \\times\n",
    " \\begin{bmatrix}\n",
    "  x_1 \\\\\n",
    "  x_2 \\\\\n",
    "  x_3\n",
    " \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构造一维列向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 有时需要构造一维列向量\n",
    "a = np.array([1, 2, 3])\n",
    "a.T # 一维行向量转置还是行向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [3]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [3]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(a, ndmin=2).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算多重神经网络的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.66376979, -0.73923643, -0.10719448, -0.48621729])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Network size\n",
    "N_input = 4\n",
    "N_hidden = 3\n",
    "N_output = 2\n",
    "\n",
    "# Make some fake data\n",
    "X = np.random.randn(4)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.15929818, -0.04339416, -0.07326922],\n",
       "        [-0.05782769, -0.01357174, -0.0793505 ],\n",
       "        [-0.036845  ,  0.01572019, -0.01154734],\n",
       "        [ 0.08317086, -0.03193551, -0.02046165]]),\n",
       " array([[ 0.02710617,  0.22448066],\n",
       "        [-0.07876019,  0.19236244],\n",
       "        [-0.11835732,  0.02289704]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初始化权重\n",
    "weights_input_to_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))\n",
    "weights_hidden_to_output = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))\n",
    "\n",
    "weights_input_to_hidden, weights_hidden_to_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_layer_in = np.dot(X, weights_input_to_hidden)\n",
    "hidden_layer_out = sigmoid(hidden_layer_in)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_out, weights_hidden_to_output)\n",
    "output_layer_out = sigmoid(output_layer_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 反向传播\n",
    "\n",
    "![](assets/img/backpropagation1.png)\n",
    "\n",
    "![](assets/img/backpropagation2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 理解反向传播算法\n",
    "\n",
    "让我们忘记数学公式和推导过程，尝试以“物理”的方式来理解这一过程。\n",
    "\n",
    "我们有一些 黄金、白银的购买数据，数据是黄金购买量，白银购买量，总价。我们需要用人脑一步步的猜测黄金、白银价格，因为每笔采购价格不同，所以无法解方程求解。\n",
    "\n",
    "假设我们是一个对黄金白银价格一无所知的人。最初的时候，我们将黄金、白银设为相同的价格，进行预测，当然会产生误差。\n",
    "\n",
    "我们发现，一笔采购中，90%是黄金，10%是白银，而误差是10000元，我们认为误差主要来自于黄金，所以我们会主要上调黄金的价格，略微上调白银价格。而另一笔采购中10%是黄金，90%是白银，误差-10000元，我们会主要下调白银价格，略微下调黄金价格。**指定维度权重调整量与指定维度输入值成正比** \n",
    "\n",
    "如果误差很小，只有100元我们对所有维度权重的调整也较小，如果误差很大，有10000元，我们对所有维度的权重的调整也会比较大。**所有权重调整量与误差成正比**\n",
    "\n",
    "另外我们有一个性格因素，就是快速的调整，还是慢慢的调整，这叫做**学习率**也是成正比的。\n",
    "\n",
    "也就是说：**维度调整量与维度输入值、误差、学习率成正比**，因此：\n",
    "\n",
    "$$ 指定维度调整量 = 学习率 \\cdot 误差项 \\cdot 指定维度输入值 $$\n",
    "\n",
    "因此**误差项是有着“物理意义”的**。\n",
    "\n",
    "$$ \\LARGE{\\delta = (y - \\hat y)f'(h) } $$\n",
    "\n",
    "如果，只是单纯的线性组合，则激活函数 $ f(x) = x $ 则 $ f'(h)=1 $，则 $\\delta = y - \\hat y $\n",
    "\n",
    "$f'(h)$ 可以看作是误差对激活函数的修正。\n",
    "\n",
    "对于多层网络，**节点的误差项来自上层节点误差的加权总和**，**上层节点误差也等于本层节点误差的加权总和**\n",
    "\n",
    "$$ \\delta_j^h = \\sum{W_{jk}\\delta_k^o f'(h_j)} $$\n",
    "\n",
    "$$ \\Delta w_{ij} = \\eta \\delta_j^h x_i $$\n",
    "\n",
    "\n",
    "这就是反向传播算法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change in weights for hidden layer to output layer:\n",
      "[ 0.00804047  0.00555918]\n",
      "Change in weights for input layer to hidden layer:\n",
      "[[  1.77005547e-04  -5.11178506e-04]\n",
      " [  3.54011093e-05  -1.02235701e-04]\n",
      " [ -7.08022187e-05   2.04471402e-04]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.5, 0.1, -0.2])\n",
    "target = 0.6\n",
    "learnrate = 0.5\n",
    "\n",
    "weights_input_hidden = np.array([[0.5, -0.6],\n",
    "                                 [0.1, -0.2],\n",
    "                                 [0.1, 0.7]])\n",
    "\n",
    "weights_hidden_output = np.array([0.1, -0.3])\n",
    "\n",
    "## Forward pass\n",
    "hidden_layer_input = np.dot(x, weights_input_hidden)\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_output, weights_hidden_output)\n",
    "output = sigmoid(output_layer_in)\n",
    "\n",
    "## Backwards pass\n",
    "## TODO: Calculate output error\n",
    "error = target - output\n",
    "\n",
    "# TODO: Calculate error term for output layer\n",
    "output_error_term = error * output * (1 - output)\n",
    "\n",
    "# TODO: Calculate error term for hidden layer\n",
    "hidden_error_term = np.dot(output_error_term, weights_hidden_output) * \\\n",
    "                    hidden_layer_output * (1 - hidden_layer_output)\n",
    "\n",
    "# TODO: Calculate change in weights for hidden layer to output layer\n",
    "delta_w_h_o = learnrate * output_error_term * hidden_layer_output\n",
    "\n",
    "# TODO: Calculate change in weights for input layer to hidden layer\n",
    "delta_w_i_h = learnrate * hidden_error_term * x[:, None]\n",
    "\n",
    "print('Change in weights for hidden layer to output layer:')\n",
    "print(delta_w_h_o)\n",
    "print('Change in weights for input layer to hidden layer:')\n",
    "print(delta_w_i_h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train loss: ', 0.22916864950741755)\n",
      "('Train loss: ', 0.22908132482042776)\n",
      "('Train loss: ', 0.2289954496785163)\n",
      "('Train loss: ', 0.2289109897070428)\n",
      "('Train loss: ', 0.22882791148045062)\n",
      "('Train loss: ', 0.2287461824925033)\n",
      "('Train loss: ', 0.22866577112757014)\n",
      "('Train loss: ', 0.2285866466329007)\n",
      "('Train loss: ', 0.22850877909186046)\n",
      "('Train loss: ', 0.2284321393980804)\n",
      "Prediction accuracy: 0.750\n"
     ]
    }
   ],
   "source": [
    "# 实现完整神经网络\n",
    "\n",
    "# Hyperparameters\n",
    "n_hidden = 2  # number of hidden units\n",
    "epochs = 900\n",
    "learnrate = 0.005\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "# Initialize weights\n",
    "weights_input_hidden = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                        size=(n_features, n_hidden))\n",
    "weights_hidden_output = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                         size=n_hidden)\n",
    "\n",
    "for e in range(epochs):\n",
    "    del_w_input_hidden = np.zeros(weights_input_hidden.shape)\n",
    "    del_w_hidden_output = np.zeros(weights_hidden_output.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        ## Forward pass ##\n",
    "        # TODO: Calculate the output\n",
    "        hidden_input = np.dot(x, weights_input_hidden)\n",
    "        hidden_output = sigmoid(hidden_input)\n",
    "\n",
    "        output = sigmoid(np.dot(hidden_output,\n",
    "                                weights_hidden_output))\n",
    "\n",
    "        ## Backward pass ##\n",
    "        # TODO: Calculate the network's prediction error\n",
    "        error = y - output\n",
    "\n",
    "        # TODO: Calculate error term for the output unit\n",
    "        output_error_term = error * output * (1 - output)\n",
    "\n",
    "        ## propagate errors to hidden layer\n",
    "\n",
    "        # TODO: Calculate the hidden layer's contribution to the error\n",
    "        hidden_error = np.dot(output_error_term, weights_hidden_output)\n",
    "\n",
    "        # TODO: Calculate the error term for the hidden layer\n",
    "        hidden_error_term = hidden_error * hidden_output * (1 - hidden_output)\n",
    "\n",
    "        # TODO: Update the change in weights\n",
    "        del_w_hidden_output += output_error_term * hidden_output\n",
    "        del_w_input_hidden += hidden_error_term * x[:, None]\n",
    "\n",
    "    # TODO: Update weights\n",
    "    weights_input_hidden += learnrate * del_w_input_hidden / n_records\n",
    "    weights_hidden_output += learnrate * del_w_hidden_output / n_records\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        hidden_output = sigmoid(np.dot(x, weights_input_hidden))\n",
    "        out = sigmoid(np.dot(hidden_output,\n",
    "                             weights_hidden_output))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "hidden = sigmoid(np.dot(features_test, weights_input_hidden))\n",
    "out = sigmoid(np.dot(hidden, weights_hidden_output))\n",
    "predictions = out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
